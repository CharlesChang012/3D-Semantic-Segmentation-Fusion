# 3D Semantic Segmentation Fusion of LiDAR and Camera (3DSSF)

This repository implements a **multi-modal 3D semantic segmentation and reconstruction system** that fuses **LiDAR point clouds and RGB images** for autonomous scene understanding.

Building on the Stanford 3D segmentation framework [1], this project integrates **DINOv3** [2] as the visual backbone in place of DINOv2 [3], evaluated on the **nuScenes** dataset [4]. 

By leveraging DINOv3â€™s enhanced visual embeddings and zero-shot capabilities, the system aims to achieve **more accurate, generalizable, and efficient 3D semantic mapping**.


## Key Highlights
- Fusion of **LiDAR (PTv3)** [5] and **image (DINOv3 or DINOv2)** features
- Evaluated on **nuScenes-Lidarseg** for real-world driving scenes  
- Includes **PointTransformerV3** integration and flexible feature fusion design

## Components
- **3D Backbone:** PointTransformerV3 (LiDAR feature encoder)  
- **2D Backbone:** DINOv3 ViT-L/16 or DINOv2 (visual feature extractor)  
- **Fusion Module:** MLP or projection-based cross-modal fusion  
- **Training:** Cross-Entropy + Lovasz-Softmax loss for per-class IoU optimization

## Setup Guide

### 1. Clone the Repository and Initialize Submodules
```bash
$ git clone git@github.com:CharlesChang012/3D-Semantic-Segmentation-Fusion.git
$ cd 3D-Semantic-Segmentation-Fusion
$ git submodule update --init --recursive
```

---

### 2. Launch an Interactive GPU Session if using UMich HPC [6] (Skip to [Step 4](#4-create-the-conda-environment) if using local machine)
Start a GPU session (see [UMich documentation](https://documentation.its.umich.edu/node/5078)):  
```bash
$ salloc --account=<account> \   
        --partition=spgpu,gpu_mig40 \  
        --nodes=1 \
        --ntasks=1 \  
        --cpus-per-task=4 \      
        --gpus=1 \ 
        --mem=16G \   
        --time=01:00:00
```

**Helper commands:**
- Check available accounts:
  ```bash
  $ my_accounts
  ```
- Monitor active jobs:
  ```bash
  $ watch squeue --me
  ```


---

### 3. Load GPU and Compiler
```bash
$ module load cuda/12.1.1
$ module load gcc/11.2.0
$ nvcc --version
```

---

### 4. Create the Conda Environment
```bash
$ cd 3D-Semantic-Segmentation-Fusion
$ conda env create -f environment.yml
$ conda activate 3DSSF
```

---

### 5. Build PointOps from PointTransformerV3 Source
```bash
cd PointTransformerV3/Pointcept/libs/pointops/
python setup.py install
```

---

## ðŸ“š References
[1] Stanford 3D Semantic Segmentation Framework  
[2] Oquab et al., *DINOv3: Self-Supervised Vision Transformers with Masked Image Modeling*, Meta AI, 2023  
[3] Oquab et al., *DINOv2: Learning Robust Visual Features without Supervision*, Meta AI, 2023  
[4] Caesar et al., *nuScenes: A Multimodal Dataset for Autonomous Driving*, CVPR 2020  
[5] Wu et al., *Point Transformer V3: Simplicity, Efficiency, and Performance in Point Cloud Transformers*, arXiv 2024  
[6] [UMich ARC Documentation](https://documentation.its.umich.edu/node/5078)
